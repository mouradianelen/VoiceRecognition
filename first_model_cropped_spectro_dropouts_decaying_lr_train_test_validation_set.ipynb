{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into train set and test set AND VALIDATION SET !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class mapping created with 2400 items.\n",
      "Train Inputs shape: torch.Size([64, 1, 64, 64])\n",
      "Train Labels: tensor([1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,\n",
      "        1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,\n",
      "        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1])\n",
      "Test Inputs shape: torch.Size([64, 1, 64, 64])\n",
      "Test Labels: tensor([1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,\n",
      "        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1])\n",
      "Validation Inputs shape: torch.Size([16, 1, 64, 64])\n",
      "Validation Labels: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from src import DAPSAudioDataset_with_multiple_cropping\n",
    "\n",
    "# Create class_mapping\n",
    "audio_dir = \"precomputed_spectrograms_aug\"\n",
    "class_1_speakers = [\"f1\", \"f7\", \"f8\", \"m3\", \"m6\", \"m8\"]\n",
    "\n",
    "class_mapping = {}\n",
    "for root, dirs, files in os.walk(audio_dir):\n",
    "    for file in files:\n",
    "        audio_path = os.path.join(root, file)\n",
    "        try:\n",
    "            speaker_prefix = file.split(\"_\")[0]\n",
    "            if speaker_prefix in class_1_speakers:\n",
    "                class_mapping[audio_path] = 1\n",
    "            else:\n",
    "                class_mapping[audio_path] = 0\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {audio_path}: {e}\")\n",
    "\n",
    "print(f\"Class mapping created with {len(class_mapping)} items.\")\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "full_dataset = DAPSAudioDataset_with_multiple_cropping(\n",
    "    class_mapping=class_mapping, num_crops=3\n",
    ")\n",
    "\n",
    "validation_ids = []\n",
    "train_test_ids = []\n",
    "\n",
    "for idx in range(len(full_dataset)):\n",
    "    path, label = full_dataset.extended_mapping[idx]\n",
    "    filename = os.path.basename(path)\n",
    "    filename_without_ext = os.path.splitext(filename)[0]\n",
    "    if \"script1\" in filename_without_ext and \"aug\" not in filename_without_ext:\n",
    "        validation_ids.append(idx)\n",
    "    else:\n",
    "        train_test_ids.append(idx)\n",
    "\n",
    "\n",
    "validation_dataset = Subset(full_dataset, validation_ids)\n",
    "\n",
    "\n",
    "def get_train_test_loaders(dataset, test_split=0.2, batch_size=64):\n",
    "    indices = list(range(len(dataset)))\n",
    "    random.shuffle(indices)\n",
    "\n",
    "    split_idx = int(len(indices) * (1 - test_split))\n",
    "    train_ids = indices[:split_idx]\n",
    "    test_ids = indices[split_idx:]\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        Subset(dataset, train_ids),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        Subset(dataset, test_ids),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "train_loader, test_loader = get_train_test_loaders(Subset(full_dataset, train_test_ids))\n",
    "\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "for batch in train_loader:\n",
    "    inputs, labels = batch\n",
    "    print(f\"Train Inputs shape: {inputs.shape}\")\n",
    "    print(f\"Train Labels: {labels}\")\n",
    "    break\n",
    "\n",
    "for batch in test_loader:\n",
    "    inputs, labels = batch\n",
    "    print(f\"Test Inputs shape: {inputs.shape}\")\n",
    "    print(f\"Test Labels: {labels}\")\n",
    "    break\n",
    "\n",
    "for batch in validation_loader:\n",
    "    inputs, labels = batch\n",
    "    print(f\"Validation Inputs shape: {inputs.shape}\")\n",
    "    print(f\"Validation Labels: {labels}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import Net "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 60, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (bn1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(60, 160, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3): Conv2d(160, 320, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (bn3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=8000, out_features=120, bias=True)\n",
      "  (dropout1): Dropout(p=0.2, inplace=False)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (dropout2): Dropout(p=0.3, inplace=False)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n",
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "freq_bins = 64\n",
    "time_steps = 64\n",
    "\n",
    "model = Net(freq_bins, time_steps)\n",
    "print(model)\n",
    "\n",
    "\n",
    "# Test forward pass\n",
    "\n",
    "test_input = torch.randn(1, 1, 64, 64)  # Simulated random input\n",
    "output = model(test_input)\n",
    "\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AVOID YOUR COMPUTER TO CRASH\n",
    "import torch\n",
    "\n",
    "torch.set_num_threads(4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de cœurs physiques: 8\n",
      "Nombre de cœurs logiques (threads disponibles): 4\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "\n",
    "print(\"Nombre de cœurs physiques:\", multiprocessing.cpu_count())\n",
    "\n",
    "\n",
    "logical_cores = torch.get_num_threads()\n",
    "print(\"Nombre de cœurs logiques (threads disponibles):\", logical_cores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, start training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cypri\\VoiceRecognition\\src\\pytorch_datasets.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  spectrogram_tensor = torch.load(spectrogram_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Loss: 0.7781\n",
      "Current learning rate: 0.001000\n",
      "Epoch 2/30, start training...\n",
      "Epoch [2/30], Loss: 0.6426\n",
      "Current learning rate: 0.001000\n",
      "Epoch 3/30, start training...\n",
      "Epoch [3/30], Loss: 0.5010\n",
      "Current learning rate: 0.001000\n",
      "Epoch 4/30, start training...\n",
      "Epoch [4/30], Loss: 0.3969\n",
      "Current learning rate: 0.001000\n",
      "Epoch 5/30, start training...\n",
      "Epoch [5/30], Loss: 0.3590\n",
      "Current learning rate: 0.001000\n",
      "Epoch 6/30, start training...\n",
      "Epoch [6/30], Loss: 0.3312\n",
      "Current learning rate: 0.001000\n",
      "Epoch 7/30, start training...\n",
      "Epoch [7/30], Loss: 0.2988\n",
      "Current learning rate: 0.001000\n",
      "Epoch 8/30, start training...\n",
      "Epoch [8/30], Loss: 0.2930\n",
      "Current learning rate: 0.001000\n",
      "Epoch 9/30, start training...\n",
      "Epoch [9/30], Loss: 0.2836\n",
      "Current learning rate: 0.001000\n",
      "Epoch 10/30, start training...\n",
      "Epoch [10/30], Loss: 0.2681\n",
      "Current learning rate: 0.001000\n",
      "Epoch 11/30, start training...\n",
      "Epoch [11/30], Loss: 0.2434\n",
      "Current learning rate: 0.000100\n",
      "Epoch 12/30, start training...\n",
      "Epoch [12/30], Loss: 0.2301\n",
      "Current learning rate: 0.000100\n",
      "Epoch 13/30, start training...\n",
      "Epoch [13/30], Loss: 0.2257\n",
      "Current learning rate: 0.000100\n",
      "Epoch 14/30, start training...\n",
      "Epoch [14/30], Loss: 0.2202\n",
      "Current learning rate: 0.000100\n",
      "Epoch 15/30, start training...\n",
      "Epoch [15/30], Loss: 0.2261\n",
      "Current learning rate: 0.000100\n",
      "Epoch 16/30, start training...\n",
      "Epoch [16/30], Loss: 0.2155\n",
      "Current learning rate: 0.000100\n",
      "Epoch 17/30, start training...\n",
      "Epoch [17/30], Loss: 0.2175\n",
      "Current learning rate: 0.000100\n",
      "Epoch 18/30, start training...\n",
      "Epoch [18/30], Loss: 0.2068\n",
      "Current learning rate: 0.000100\n",
      "Epoch 19/30, start training...\n",
      "Epoch [19/30], Loss: 0.2031\n",
      "Current learning rate: 0.000100\n",
      "Epoch 20/30, start training...\n",
      "Epoch [20/30], Loss: 0.1998\n",
      "Current learning rate: 0.000100\n",
      "Epoch 21/30, start training...\n",
      "Epoch [21/30], Loss: 0.2019\n",
      "Current learning rate: 0.000010\n",
      "Epoch 22/30, start training...\n",
      "Epoch [22/30], Loss: 0.1984\n",
      "Current learning rate: 0.000010\n",
      "Epoch 23/30, start training...\n",
      "Epoch [23/30], Loss: 0.2000\n",
      "Current learning rate: 0.000010\n",
      "Epoch 24/30, start training...\n",
      "Epoch [24/30], Loss: 0.1993\n",
      "Current learning rate: 0.000010\n",
      "Epoch 25/30, start training...\n",
      "Epoch [25/30], Loss: 0.1915\n",
      "Current learning rate: 0.000010\n",
      "Epoch 26/30, start training...\n",
      "Epoch [26/30], Loss: 0.1994\n",
      "Current learning rate: 0.000010\n",
      "Epoch 27/30, start training...\n",
      "Epoch [27/30], Loss: 0.1926\n",
      "Current learning rate: 0.000010\n",
      "Epoch 28/30, start training...\n",
      "Epoch [28/30], Loss: 0.1866\n",
      "Current learning rate: 0.000010\n",
      "Epoch 29/30, start training...\n",
      "Epoch [29/30], Loss: 0.1994\n",
      "Current learning rate: 0.000010\n",
      "Epoch 30/30, start training...\n",
      "Epoch [30/30], Loss: 0.1936\n",
      "Current learning rate: 0.000010\n",
      "Model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from src import Net\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "freq_bins = 64\n",
    "time_steps = 64\n",
    "\n",
    "\n",
    "model = Net(freq_bins, time_steps).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  \n",
    "\n",
    "\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "\n",
    "num_epochs = 30\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, start training...\")\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"Current learning rate: {current_lr:.6f}\")\n",
    "\n",
    "    \n",
    "    scheduler.step() \n",
    "    \n",
    "\n",
    "torch.save(\n",
    "    model.state_dict(),\n",
    "    f\"simple_cnn_model_reduced_first_layer_decaying_lr{freq_bins}x{time_steps}.pth\",\n",
    ")\n",
    "print(\"Model saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cypri\\AppData\\Local\\Temp\\ipykernel_41176\\4025631402.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(\n",
      "c:\\Users\\cypri\\VoiceRecognition\\src\\pytorch_datasets.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  spectrogram_tensor = torch.load(spectrogram_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the model is loaded\n",
      "start testing occurence 1/20\n",
      "start testing occurence 2/20\n",
      "start testing occurence 3/20\n",
      "start testing occurence 4/20\n",
      "start testing occurence 5/20\n",
      "start testing occurence 6/20\n",
      "start testing occurence 7/20\n",
      "start testing occurence 8/20\n",
      "start testing occurence 9/20\n",
      "start testing occurence 10/20\n",
      "start testing occurence 11/20\n",
      "start testing occurence 12/20\n",
      "start testing occurence 13/20\n",
      "start testing occurence 14/20\n",
      "start testing occurence 15/20\n",
      "start testing occurence 16/20\n",
      "start testing occurence 17/20\n",
      "start testing occurence 18/20\n",
      "start testing occurence 19/20\n",
      "start testing occurence 20/20\n",
      "F1-Score: 0.9027\n",
      "Precision: 0.9707\n",
      "Recall: 0.8435\n",
      "Accuracy: 0.8865\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "from src import Net\n",
    "freq_bins = 64\n",
    "time_steps = 64\n",
    "\n",
    "model = Net(freq_bins, time_steps)\n",
    "model.load_state_dict(\n",
    "    torch.load(\n",
    "        f\"simple_cnn_model_reduced_first_layer_decaying_lr{freq_bins}x{time_steps}.pth\"\n",
    "    )\n",
    ")\n",
    "print(\"the model is loaded\")\n",
    "model.eval()\n",
    "\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "\n",
    "occurence = 0\n",
    "with torch.no_grad():\n",
    "    length = len(test_loader)\n",
    "    for inputs, labels in test_loader:\n",
    "        print(f\"start testing occurence {occurence + 1}/{length}\")\n",
    "        occurence += 1\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "f1 = f1_score(all_labels, all_predictions, average=\"binary\")\n",
    "precision = precision_score(all_labels, all_predictions, average=\"binary\")\n",
    "recall = recall_score(all_labels, all_predictions, average=\"binary\")\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "\n",
    "# Print metrics\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cypri\\AppData\\Local\\Temp\\ipykernel_41176\\1271331471.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(\n",
      "c:\\Users\\cypri\\VoiceRecognition\\src\\pytorch_datasets.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  spectrogram_tensor = torch.load(spectrogram_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is loaded.\n",
      "Start testing occurrence 1/57\n",
      "Start testing occurrence 2/57\n",
      "Start testing occurrence 3/57\n",
      "Start testing occurrence 4/57\n",
      "Start testing occurrence 5/57\n",
      "Start testing occurrence 6/57\n",
      "Start testing occurrence 7/57\n",
      "Start testing occurrence 8/57\n",
      "Start testing occurrence 9/57\n",
      "Start testing occurrence 10/57\n",
      "Start testing occurrence 11/57\n",
      "Start testing occurrence 12/57\n",
      "Start testing occurrence 13/57\n",
      "Start testing occurrence 14/57\n",
      "Start testing occurrence 15/57\n",
      "Start testing occurrence 16/57\n",
      "Start testing occurrence 17/57\n",
      "Start testing occurrence 18/57\n",
      "Start testing occurrence 19/57\n",
      "Start testing occurrence 20/57\n",
      "Start testing occurrence 21/57\n",
      "Start testing occurrence 22/57\n",
      "Start testing occurrence 23/57\n",
      "Start testing occurrence 24/57\n",
      "Start testing occurrence 25/57\n",
      "Start testing occurrence 26/57\n",
      "Start testing occurrence 27/57\n",
      "Start testing occurrence 28/57\n",
      "Start testing occurrence 29/57\n",
      "Start testing occurrence 30/57\n",
      "Start testing occurrence 31/57\n",
      "Start testing occurrence 32/57\n",
      "Start testing occurrence 33/57\n",
      "Start testing occurrence 34/57\n",
      "Start testing occurrence 35/57\n",
      "Start testing occurrence 36/57\n",
      "Start testing occurrence 37/57\n",
      "Start testing occurrence 38/57\n",
      "Start testing occurrence 39/57\n",
      "Start testing occurrence 40/57\n",
      "Start testing occurrence 41/57\n",
      "Start testing occurrence 42/57\n",
      "Start testing occurrence 43/57\n",
      "Start testing occurrence 44/57\n",
      "Start testing occurrence 45/57\n",
      "Start testing occurrence 46/57\n",
      "Start testing occurrence 47/57\n",
      "Start testing occurrence 48/57\n",
      "Start testing occurrence 49/57\n",
      "Start testing occurrence 50/57\n",
      "Start testing occurrence 51/57\n",
      "Start testing occurrence 52/57\n",
      "Start testing occurrence 53/57\n",
      "Start testing occurrence 54/57\n",
      "Start testing occurrence 55/57\n",
      "Start testing occurrence 56/57\n",
      "Start testing occurrence 57/57\n",
      "F1-Score: 0.8106\n",
      "Precision: 0.9005\n",
      "Recall: 0.7370\n",
      "Accuracy: 0.8967\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "from src import Net\n",
    "\n",
    "freq_bins = 64\n",
    "time_steps = 64\n",
    "\n",
    "model = Net(freq_bins, time_steps)\n",
    "model.load_state_dict(\n",
    "    torch.load(\n",
    "        f\"simple_cnn_model_reduced_first_layer_decaying_lr{freq_bins}x{time_steps}.pth\"\n",
    "    )\n",
    ")\n",
    "print(\"The model is loaded.\")\n",
    "model.eval()\n",
    "\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    length = len(validation_loader)\n",
    "    for occurence, (inputs, labels) in enumerate(validation_loader, 1):\n",
    "        print(f\"Start testing occurrence {occurence}/{length}\")\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "# Calculate metrics\n",
    "f1 = f1_score(all_labels, all_predictions, average=\"binary\")\n",
    "precision = precision_score(all_labels, all_predictions, average=\"binary\")\n",
    "recall = recall_score(all_labels, all_predictions, average=\"binary\")\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "\n",
    "# Print metrics\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch_Intro_ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
