{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into train set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class mapping created:\n",
      "Total Class 0 samples: 1050\n",
      "Total Class 1 samples: 900\n",
      "Preparing the dataset...\n",
      "300\n",
      "1560\n",
      "Dataset created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\WUT\\ML\\MLProject\\src\\pytorch_datasets.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  spectrogram_tensor = torch.load(spectrogram_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Inputs shape: torch.Size([64, 1, 64, 64])\n",
      "Train Labels: tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,\n",
      "        1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,\n",
      "        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1])\n",
      "Test Inputs shape: torch.Size([16, 1, 64, 64])\n",
      "Test Labels: tensor([0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "from src import DAPSAudioDataset\n",
    "import os\n",
    "import librosa\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "import torch\n",
    "\n",
    "audio_dir = \"daps\"\n",
    "\n",
    "class_1_speakers = [\"f1\", \"f7\", \"f8\", \"m3\", \"m6\", \"m8\"]\n",
    "\n",
    "class_mapping = {}\n",
    "\n",
    "for root, dirs, files in os.walk('precomputed_spectrograms_aug_2'):\n",
    "    for file in files:\n",
    "        audio_path = os.path.join(root, file)\n",
    "        try:\n",
    "            speaker_prefix = file.split(\"_\")[0]\n",
    "            if speaker_prefix in class_1_speakers:\n",
    "                class_mapping[audio_path] = 1\n",
    "            else:\n",
    "                class_mapping[audio_path] = 0\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {audio_path}: {e}\")\n",
    "\n",
    "print(\"Class mapping created:\")\n",
    "\n",
    "class_0_count = sum(1 for label in class_mapping.values() if label == 0)\n",
    "class_1_count = sum(1 for label in class_mapping.values() if label == 1)\n",
    "print(f\"Total Class 0 samples: {class_0_count}\")\n",
    "print(f\"Total Class 1 samples: {class_1_count}\")\n",
    "print(\"Preparing the dataset...\")\n",
    "\n",
    "full_dataset = DAPSAudioDataset(class_mapping=class_mapping)\n",
    "\n",
    "test_ids =[]\n",
    "train_ids =[]\n",
    "filenames = set()\n",
    "f = open(\"demofile2.txt\", \"a\")\n",
    "for idx in range(len(full_dataset)):\n",
    "    path, label = full_dataset.class_mapping[idx]\n",
    "    if path is not None:\n",
    "        filename = os.path.basename(path)\n",
    "            \n",
    "        filename_without_ext = os.path.splitext(filename)[0]\n",
    "        parts = filename_without_ext.split('_')\n",
    "            \n",
    "        group = parts[0]\n",
    "        place = \"\"\n",
    "        if 'script1' in filename_without_ext:\n",
    "            if 'aug' not in filename_without_ext:\n",
    "                filenames.add(place)\n",
    "                test_ids.append(idx)\n",
    "            else: \n",
    "                continue\n",
    "        else:\n",
    "            train_ids.append(idx) \n",
    "            f.write(f\"{filename_without_ext}\\n\")\n",
    "print(len(test_ids))\n",
    "print(len(train_ids))\n",
    "f.close()\n",
    "\n",
    "print(\"Dataset created\")\n",
    "\n",
    "train_dataset = Subset(full_dataset, train_ids)\n",
    "test_dataset = Subset(full_dataset, test_ids)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    if len(batch) == 0:\n",
    "        return None\n",
    "    return torch.utils.data.dataloader.default_collate(batch)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=64, shuffle=True, collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "for batch in train_loader:\n",
    "    inputs, labels = batch\n",
    "    print(f\"Train Inputs shape: {inputs.shape}\")\n",
    "    print(f\"Train Labels: {labels}\")\n",
    "    break\n",
    "\n",
    "for batch in test_loader:\n",
    "    inputs, labels = batch\n",
    "    print(f\"Test Inputs shape: {inputs.shape}\")\n",
    "    print(f\"Test Labels: {labels}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from src import Net "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output size after all conv and pool layers: 27040\n",
      "Net(\n",
      "  (conv1): Conv2d(1, 60, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(60, 160, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=27040, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n",
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "model = Net()\n",
    "print(model)\n",
    "\n",
    "# Test forward pass\n",
    "test_input = torch.randn(1, 1, 64, 64)  # Simulated random input\n",
    "output = model(test_input)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### reduce the train set to few samples for faster training\n",
    "### only for tests/ reduces the efficiency of the model\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "\n",
    "subset_dataset = Subset(train_dataset, list(range(20)))\n",
    "\n",
    "train_loader = DataLoader(subset_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AVOID YOUR COMPUTER TO CRASH\n",
    "import torch\n",
    "\n",
    "torch.set_num_threads(4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de cœurs physiques: 8\n",
      "Nombre de cœurs logiques (threads disponibles): 4\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "\n",
    "print(\"Nombre de cœurs physiques:\", multiprocessing.cpu_count())\n",
    "\n",
    "\n",
    "logical_cores = torch.get_num_threads()\n",
    "print(\"Nombre de cœurs logiques (threads disponibles):\", logical_cores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output size after all conv and pool layers: 27040\n",
      "Epoch 1/80, start training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\WUT\\ML\\MLProject\\src\\pytorch_datasets.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  spectrogram_tensor = torch.load(spectrogram_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/80], Loss: 0.8677\n",
      "Epoch 2/80, start training...\n",
      "Epoch [2/80], Loss: 0.5781\n",
      "Epoch 3/80, start training...\n",
      "Epoch [3/80], Loss: 0.4811\n",
      "Epoch 4/80, start training...\n",
      "Epoch [4/80], Loss: 0.4449\n",
      "Epoch 5/80, start training...\n",
      "Epoch [5/80], Loss: 0.4140\n",
      "Epoch 6/80, start training...\n",
      "Epoch [6/80], Loss: 0.4317\n",
      "Epoch 7/80, start training...\n",
      "Epoch [7/80], Loss: 0.3913\n",
      "Epoch 8/80, start training...\n",
      "Epoch [8/80], Loss: 0.4065\n",
      "Epoch 9/80, start training...\n",
      "Epoch [9/80], Loss: 0.4079\n",
      "Epoch 10/80, start training...\n",
      "Epoch [10/80], Loss: 0.3818\n",
      "Epoch 11/80, start training...\n",
      "Epoch [11/80], Loss: 0.3798\n",
      "Epoch 12/80, start training...\n",
      "Epoch [12/80], Loss: 0.3685\n",
      "Epoch 13/80, start training...\n",
      "Epoch [13/80], Loss: 0.3584\n",
      "Epoch 14/80, start training...\n",
      "Epoch [14/80], Loss: 0.3545\n",
      "Epoch 15/80, start training...\n",
      "Epoch [15/80], Loss: 0.3729\n",
      "Epoch 16/80, start training...\n",
      "Epoch [16/80], Loss: 0.3510\n",
      "Epoch 17/80, start training...\n",
      "Epoch [17/80], Loss: 0.3566\n",
      "Epoch 18/80, start training...\n",
      "Epoch [18/80], Loss: 0.3403\n",
      "Epoch 19/80, start training...\n",
      "Epoch [19/80], Loss: 0.3281\n",
      "Epoch 20/80, start training...\n",
      "Epoch [20/80], Loss: 0.2999\n",
      "Epoch 21/80, start training...\n",
      "Epoch [21/80], Loss: 0.3142\n",
      "Epoch 22/80, start training...\n",
      "Epoch [22/80], Loss: 0.2829\n",
      "Epoch 23/80, start training...\n",
      "Epoch [23/80], Loss: 0.3111\n",
      "Epoch 24/80, start training...\n",
      "Epoch [24/80], Loss: 0.2862\n",
      "Epoch 25/80, start training...\n",
      "Epoch [25/80], Loss: 0.3054\n",
      "Epoch 26/80, start training...\n",
      "Epoch [26/80], Loss: 0.2978\n",
      "Epoch 27/80, start training...\n",
      "Epoch [27/80], Loss: 0.2851\n",
      "Epoch 28/80, start training...\n",
      "Epoch [28/80], Loss: 0.3084\n",
      "Epoch 29/80, start training...\n",
      "Epoch [29/80], Loss: 0.2658\n",
      "Epoch 30/80, start training...\n",
      "Epoch [30/80], Loss: 0.2618\n",
      "Epoch 31/80, start training...\n",
      "Epoch [31/80], Loss: 0.2653\n",
      "Epoch 32/80, start training...\n",
      "Epoch [32/80], Loss: 0.2511\n",
      "Epoch 33/80, start training...\n",
      "Epoch [33/80], Loss: 0.2613\n",
      "Epoch 34/80, start training...\n",
      "Epoch [34/80], Loss: 0.2624\n",
      "Epoch 35/80, start training...\n",
      "Epoch [35/80], Loss: 0.2481\n",
      "Epoch 36/80, start training...\n",
      "Epoch [36/80], Loss: 0.2405\n",
      "Epoch 37/80, start training...\n",
      "Epoch [37/80], Loss: 0.2306\n",
      "Epoch 38/80, start training...\n",
      "Epoch [38/80], Loss: 0.2361\n",
      "Epoch 39/80, start training...\n",
      "Epoch [39/80], Loss: 0.2460\n",
      "Epoch 40/80, start training...\n",
      "Epoch [40/80], Loss: 0.2354\n",
      "Epoch 41/80, start training...\n",
      "Epoch [41/80], Loss: 0.2255\n",
      "Epoch 42/80, start training...\n",
      "Epoch [42/80], Loss: 0.2270\n",
      "Epoch 43/80, start training...\n",
      "Epoch [43/80], Loss: 0.2431\n",
      "Epoch 44/80, start training...\n",
      "Epoch [44/80], Loss: 0.2219\n",
      "Epoch 45/80, start training...\n",
      "Epoch [45/80], Loss: 0.2324\n",
      "Epoch 46/80, start training...\n",
      "Epoch [46/80], Loss: 0.2213\n",
      "Epoch 47/80, start training...\n",
      "Epoch [47/80], Loss: 0.2294\n",
      "Epoch 48/80, start training...\n",
      "Epoch [48/80], Loss: 0.2336\n",
      "Epoch 49/80, start training...\n",
      "Epoch [49/80], Loss: 0.2519\n",
      "Epoch 50/80, start training...\n",
      "Epoch [50/80], Loss: 0.2471\n",
      "Epoch 51/80, start training...\n",
      "Epoch [51/80], Loss: 0.2170\n",
      "Epoch 52/80, start training...\n",
      "Epoch [52/80], Loss: 0.2227\n",
      "Epoch 53/80, start training...\n",
      "Epoch [53/80], Loss: 0.2077\n",
      "Epoch 54/80, start training...\n",
      "Epoch [54/80], Loss: 0.2318\n",
      "Epoch 55/80, start training...\n",
      "Epoch [55/80], Loss: 0.2207\n",
      "Epoch 56/80, start training...\n",
      "Epoch [56/80], Loss: 0.2206\n",
      "Epoch 57/80, start training...\n",
      "Epoch [57/80], Loss: 0.2048\n",
      "Epoch 58/80, start training...\n",
      "Epoch [58/80], Loss: 0.1949\n",
      "Epoch 59/80, start training...\n",
      "Epoch [59/80], Loss: 0.2111\n",
      "Epoch 60/80, start training...\n",
      "Epoch [60/80], Loss: 0.2087\n",
      "Epoch 61/80, start training...\n",
      "Epoch [61/80], Loss: 0.2116\n",
      "Epoch 62/80, start training...\n",
      "Epoch [62/80], Loss: 0.2066\n",
      "Epoch 63/80, start training...\n",
      "Epoch [63/80], Loss: 0.2073\n",
      "Epoch 64/80, start training...\n",
      "Epoch [64/80], Loss: 0.1981\n",
      "Epoch 65/80, start training...\n",
      "Epoch [65/80], Loss: 0.1924\n",
      "Epoch 66/80, start training...\n",
      "Epoch [66/80], Loss: 0.2009\n",
      "Epoch 67/80, start training...\n",
      "Epoch [67/80], Loss: 0.2101\n",
      "Epoch 68/80, start training...\n",
      "Epoch [68/80], Loss: 0.1952\n",
      "Epoch 69/80, start training...\n",
      "Epoch [69/80], Loss: 0.2167\n",
      "Epoch 70/80, start training...\n",
      "Epoch [70/80], Loss: 0.1964\n",
      "Epoch 71/80, start training...\n",
      "Epoch [71/80], Loss: 0.1890\n",
      "Epoch 72/80, start training...\n",
      "Epoch [72/80], Loss: 0.1894\n",
      "Epoch 73/80, start training...\n",
      "Epoch [73/80], Loss: 0.1941\n",
      "Epoch 74/80, start training...\n",
      "Epoch [74/80], Loss: 0.2244\n",
      "Epoch 75/80, start training...\n",
      "Epoch [75/80], Loss: 0.2008\n",
      "Epoch 76/80, start training...\n",
      "Epoch [76/80], Loss: 0.1981\n",
      "Epoch 77/80, start training...\n",
      "Epoch [77/80], Loss: 0.2098\n",
      "Epoch 78/80, start training...\n",
      "Epoch [78/80], Loss: 0.2113\n",
      "Epoch 79/80, start training...\n",
      "Epoch [79/80], Loss: 0.1833\n",
      "Epoch 80/80, start training...\n",
      "Epoch [80/80], Loss: 0.2101\n",
      "Model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from src import Net\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = Net().to(device)  \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  \n",
    "\n",
    "num_epochs = 80\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, start training...\")\n",
    "    model.train()  \n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)  \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()  \n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "torch.save(model.state_dict(), \"simple_cnn_model_reduced.pth\")\n",
    "print(\"Model saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output size after all conv and pool layers: 27040\n",
      "the model is loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\elenm\\AppData\\Local\\Temp\\ipykernel_22220\\506085010.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"simple_cnn_model_reduced.pth\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start testing occurence 1/19\n",
      "start testing occurence 2/19\n",
      "start testing occurence 3/19\n",
      "start testing occurence 4/19\n",
      "start testing occurence 5/19\n",
      "start testing occurence 6/19\n",
      "start testing occurence 7/19\n",
      "start testing occurence 8/19\n",
      "start testing occurence 9/19\n",
      "start testing occurence 10/19\n",
      "start testing occurence 11/19\n",
      "start testing occurence 12/19\n",
      "start testing occurence 13/19\n",
      "start testing occurence 14/19\n",
      "start testing occurence 15/19\n",
      "start testing occurence 16/19\n",
      "start testing occurence 17/19\n",
      "start testing occurence 18/19\n",
      "start testing occurence 19/19\n",
      "F1-Score: 0.9029\n",
      "Precision: 0.9294\n",
      "Recall: 0.8778\n",
      "Accuracy: 0.9433\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "from src import Net\n",
    "model = Net()\n",
    "\n",
    "model.load_state_dict(torch.load(\"simple_cnn_model_reduced.pth\"))\n",
    "print(\"the model is loaded\")\n",
    "model.eval()\n",
    "\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "\n",
    "occurence = 0\n",
    "with torch.no_grad():\n",
    "    length = len(test_loader)\n",
    "    for inputs, labels in test_loader:\n",
    "        print(f\"start testing occurence {occurence + 1}/{length}\")\n",
    "        occurence += 1\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(\n",
    "            outputs, 1\n",
    "        )  \n",
    "\n",
    "        \n",
    "        all_labels.extend(labels.cpu().numpy())  \n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "f1 = f1_score(all_labels, all_predictions, average=\"binary\")\n",
    "precision = precision_score(all_labels, all_predictions, average=\"binary\")\n",
    "recall = recall_score(all_labels, all_predictions, average=\"binary\")\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "\n",
    "# Print metrics\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
