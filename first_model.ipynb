{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into train set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class mapping created:\n",
      "Total Class 0 samples: 1050\n",
      "Total Class 1 samples: 450\n",
      "Preparing the dataset...\n",
      "Dataset created\n",
      "one spectro is computed\n",
      "one spectro is computed\n",
      "one spectro is computed\n",
      "one spectro is computed\n",
      "one spectro is computed\n",
      "one spectro is computed\n",
      "one spectro is computed\n",
      "one spectro is computed\n",
      "one spectro is computed\n",
      "one spectro is computed\n",
      "one spectro is computed\n",
      "one spectro is computed\n",
      "one spectro is computed\n",
      "one spectro is computed\n",
      "one spectro is computed\n",
      "one spectro is computed\n",
      "Train Inputs shape: torch.Size([16, 1, 128, 16000])\n",
      "Train Labels: tensor([1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1])\n",
      "Validation Inputs shape: torch.Size([16, 1, 128, 16000])\n",
      "Validation Labels: tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0])\n",
      "Test Inputs shape: torch.Size([16, 1, 128, 16000])\n",
      "Test Labels: tensor([0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from dap_datasets import DAPSAudioDataset\n",
    "import torch\n",
    "\n",
    "audio_dir = \"daps\"\n",
    "\n",
    "class_1_speakers = [\"f1\", \"f7\", \"f8\", \"m3\", \"m6\", \"m8\"]\n",
    "\n",
    "class_mapping = {}\n",
    "\n",
    "for root, dirs, files in os.walk(audio_dir):\n",
    "    for file in files:\n",
    "        # Skip files that start with \"._\" or are not \".wav\" files\n",
    "        if file.startswith(\"._\") or not file.endswith(\".wav\"):\n",
    "            continue\n",
    "\n",
    "        audio_path = os.path.join(root, file)\n",
    "        try:\n",
    "            audio, sr = librosa.load(audio_path, sr=None)\n",
    "            speaker_prefix = file.split(\"_\")[0]\n",
    "            if speaker_prefix in class_1_speakers:\n",
    "                class_mapping[audio_path] = 1\n",
    "            else:\n",
    "                class_mapping[audio_path] = 0\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {audio_path}: {e}\")\n",
    "\n",
    "print(\"Class mapping created:\")\n",
    "#print(class_mapping)\n",
    "\n",
    "class_0_count = sum(1 for label in class_mapping.values() if label == 0)\n",
    "class_1_count = sum(1 for label in class_mapping.values() if label == 1)\n",
    "\n",
    "print(f\"Total Class 0 samples: {class_0_count}\")\n",
    "print(f\"Total Class 1 samples: {class_1_count}\")\n",
    "\n",
    "def preprocess_audio(audio_path, max_length=16000):\n",
    "    try:\n",
    "        audio, sr = librosa.load(audio_path, sr=None)\n",
    "        if audio is None or len(audio) == 0:\n",
    "            return None\n",
    "\n",
    "        mel_spectrogram = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=128, fmax=8000)\n",
    "        mel_spectrogram_db = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "        normalized_spectrogram = (mel_spectrogram_db - np.mean(mel_spectrogram_db)) / np.std(mel_spectrogram_db)\n",
    "\n",
    "        target_length = max_length\n",
    "        if normalized_spectrogram.shape[1] > target_length:\n",
    "            normalized_spectrogram = normalized_spectrogram[:, :target_length]\n",
    "        else:\n",
    "            padding = target_length - normalized_spectrogram.shape[1]\n",
    "            normalized_spectrogram = np.pad(normalized_spectrogram, ((0, 0), (0, padding)), mode=\"constant\")\n",
    "\n",
    "        spectrogram_tensor = torch.tensor(normalized_spectrogram, dtype=torch.float32).unsqueeze(0)\n",
    "        return spectrogram_tensor\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {audio_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "class DAPSAudioDataset:\n",
    "    def __init__(self, class_mapping, transform=None):\n",
    "        self.class_mapping = list(class_mapping.items())\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.class_mapping)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_path, label = self.class_mapping[idx]\n",
    "        processed_audio = self.transform(audio_path) if self.transform else None\n",
    "\n",
    "        if processed_audio is None:\n",
    "            return None  \n",
    "\n",
    "        return processed_audio, label\n",
    "\n",
    "print(\"Preparing the dataset...\")\n",
    "full_dataset = DAPSAudioDataset(class_mapping=class_mapping, transform=preprocess_audio)\n",
    "print(\"Dataset created\")\n",
    "\n",
    "train_ratio = 0.8\n",
    "val_ratio = 0.1\n",
    "train_size = int(train_ratio * len(full_dataset))\n",
    "val_size = int(val_ratio * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(full_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    if len(batch) == 0:\n",
    "        return None\n",
    "    return torch.utils.data.dataloader.default_collate(batch)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "for batch in train_loader:\n",
    "    inputs, labels = batch\n",
    "    print(f\"Train Inputs shape: {inputs.shape}\")\n",
    "    print(f\"Train Labels: {labels}\")\n",
    "    break\n",
    "\n",
    "for batch in val_loader:\n",
    "    inputs, labels = batch\n",
    "    print(f\"Validation Inputs shape: {inputs.shape}\")\n",
    "    print(f\"Validation Labels: {labels}\")\n",
    "    break\n",
    "\n",
    "for batch in test_loader:\n",
    "    inputs, labels = batch\n",
    "    print(f\"Test Inputs shape: {inputs.shape}\")\n",
    "    print(f\"Test Labels: {labels}\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loader))\n",
    "print(len(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)  \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        \n",
    "    \n",
    "        self.fc1_input_size = self._get_fc1_input_size()\n",
    "        self.fc1 = nn.Linear(self.fc1_input_size, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)  \n",
    "\n",
    "    def _get_fc1_input_size(self):\n",
    "        with torch.no_grad():\n",
    "            x = torch.zeros(1, 1, 128, 16000)  \n",
    "            x = self.pool(F.relu(self.conv1(x)))\n",
    "            x = self.pool(F.relu(self.conv2(x)))\n",
    "            return x.numel()  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)  \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "### reduce the train set to few samples for faster training\n",
    "### only for tests/ reduces the efficiency of the model\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "\n",
    "subset_dataset = Subset(train_dataset, list(range(20)))\n",
    "\n",
    "train_loader = DataLoader(subset_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AVOID YOUR COMPUTER TO CRASH\n",
    "import torch\n",
    "\n",
    "torch.set_num_threads(4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de cœurs physiques: 8\n",
      "Nombre de cœurs logiques (threads disponibles): 4\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "\n",
    "print(\"Nombre de cœurs physiques:\", multiprocessing.cpu_count())\n",
    "\n",
    "\n",
    "logical_cores = torch.get_num_threads()\n",
    "print(\"Nombre de cœurs logiques (threads disponibles):\", logical_cores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, start training...\n",
      "Epoch [1/10], Loss: 2.2052\n",
      "Epoch 2/10, start training...\n",
      "Epoch [2/10], Loss: 0.0185\n",
      "Epoch 3/10, start training...\n",
      "Epoch [3/10], Loss: 0.0014\n",
      "Epoch 4/10, start training...\n",
      "Epoch [4/10], Loss: 0.0006\n",
      "Epoch 5/10, start training...\n",
      "Epoch [5/10], Loss: 0.0001\n",
      "Epoch 6/10, start training...\n",
      "Epoch [6/10], Loss: 0.0001\n",
      "Epoch 7/10, start training...\n",
      "Epoch [7/10], Loss: 0.0001\n",
      "Epoch 8/10, start training...\n",
      "Epoch [8/10], Loss: 0.0000\n",
      "Epoch 9/10, start training...\n",
      "Epoch [9/10], Loss: 0.0000\n",
      "Epoch 10/10, start training...\n",
      "Epoch [10/10], Loss: 0.0000\n",
      "Model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = Net().to(device)  \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  \n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, start training...\")\n",
    "    model.train()  \n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)  \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()  \n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "torch.save(model.state_dict(), \"simple_cnn_model.pth\")\n",
    "print(\"Model saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\elenm\\AppData\\Local\\Temp\\ipykernel_21548\\622806710.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"simple_cnn_model.pth\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the model is loaded\n",
      "start testing occurence 0/10\n",
      "start testing occurence 1/10\n",
      "start testing occurence 2/10\n",
      "start testing occurence 3/10\n",
      "start testing occurence 4/10\n",
      "start testing occurence 5/10\n",
      "start testing occurence 6/10\n",
      "start testing occurence 7/10\n",
      "start testing occurence 8/10\n",
      "start testing occurence 9/10\n",
      "F1-Score: 1.0000\n",
      "Precision: 1.0000\n",
      "Recall: 1.0000\n",
      "Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "model = Net()\n",
    "\n",
    "model.load_state_dict(torch.load(\"simple_cnn_model.pth\"))\n",
    "print(\"the model is loaded\")\n",
    "model.eval()\n",
    "\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "\n",
    "occurence = 0\n",
    "with torch.no_grad():\n",
    "    length = len(test_loader)\n",
    "    for inputs, labels in test_loader:\n",
    "        print(f\"start testing occurence {occurence}/{length}\")\n",
    "        occurence += 1\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(\n",
    "            outputs, 1\n",
    "        )  \n",
    "\n",
    "        \n",
    "        all_labels.extend(labels.cpu().numpy())  \n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "f1 = f1_score(all_labels, all_predictions, average=\"binary\")\n",
    "precision = precision_score(all_labels, all_predictions, average=\"binary\")\n",
    "recall = recall_score(all_labels, all_predictions, average=\"binary\")\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "\n",
    "# Print metrics\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
